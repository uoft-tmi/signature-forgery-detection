{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1U3fGRQgDYO"
      },
      "source": [
        "#Get Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle Library\n",
        "!pip install kaggle\n",
        "\n",
        "# Before next step, user needs to download the free API KEY from Kaggle settings\n",
        "# Upload the kaggle.json file to Google Colab Files\n",
        "\n",
        "# Make directory for Kaggle & Refer to API KEY\n",
        "! mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lMI9nZrabi0",
        "outputId": "3c5146aa-3f55-4fa7-8206-254c5c99b6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_b_YWzhq9VS",
        "outputId": "e68a9fec-4ce9-4a95-8329-3850856dc289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading signature-verification-dataset.zip to /content\n",
            " 98% 588M/601M [00:06<00:00, 47.3MB/s]\n",
            "100% 601M/601M [00:06<00:00, 93.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download Dataset\n",
        "! kaggle datasets download -d robinreni/signature-verification-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld1wPTXYrCsA"
      },
      "outputs": [],
      "source": [
        "# ! mkdir sfddata\n",
        "! unzip signature-verification-dataset.zip -d sfddata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete Folder"
      ],
      "metadata": {
        "id": "xDNq3O72Hgzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'reduced' with the name of your folder\n",
        "folder_to_delete = 'reduced'\n",
        "\n",
        "# Use shutil.rmtree to delete the folder and its contents\n",
        "shutil.rmtree(folder_to_delete, ignore_errors=True)"
      ],
      "metadata": {
        "id": "1Z9eP3hmHdIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "KwsNMpFLoyw-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJup25ZtSqKR"
      },
      "source": [
        "## Convert Image to Grayscale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTBDjNcfSnO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683c9144-7ba1-4b9d-a379-165a5acb7e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:00<00:00, 182.37it/s]\n",
            "100%|██████████| 12/12 [00:00<00:00, 90.24it/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Code is used for processing images\"\"\"\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import numpy\n",
        "\n",
        "THRESHOLD = 128\n",
        "\n",
        "def image_to_grayscale(image_dir: str) -> Image:\n",
        "    \"\"\"Used for testing purposes to convert one image to grayscale\"\"\"\n",
        "    image = Image.open(image_dir)\n",
        "    gray_image = ImageOps.grayscale(image)\n",
        "    return gray_image\n",
        "\n",
        "def convert_grayscale(directory: str) -> list:\n",
        "    \"\"\"Converts all images in the given directory into gray scale\"\"\"\n",
        "    converted_images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".png\") or filename.endswith(\".PNG\"):\n",
        "            image = Image.open(directory + \"/\" + filename)\n",
        "            gray_image = ImageOps.grayscale(image)\n",
        "            converted_images.append(gray_image)\n",
        "    return converted_images\n",
        "\n",
        "def reduce_noise(directory: str, image_path: str) -> list:\n",
        "    \"\"\"Runs fastNLMeansDenoising on each img\"\"\"\n",
        "    reduced_dir = \"/content/reduced/\" + directory\n",
        "    os.makedirs(image_path, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".png\") or filename.endswith(\".PNG\"):\n",
        "          noise_pic = cv2.imread(directory + \"/\" + filename)\n",
        "          image_again = numpy.asarray(noise_pic)\n",
        "          less_noise_pic = cv2.fastNlMeansDenoising(image_again, None, 15, 7, 21)\n",
        "          curr_path = os.path.join(image_path, f\"{filename}\")\n",
        "          cv2.imwrite(curr_path, less_noise_pic)\n",
        "\n",
        "def resize(images: list, dimensions: tuple) -> None:\n",
        "    \"\"\"Resizes all the given images in a list\"\"\"\n",
        "    for i in range(0, len(images)):\n",
        "        image = images[i]\n",
        "        images[i] = image.resize(dimensions)\n",
        "\n",
        "# actual code to run\n",
        "dimensions = (250, 250) # dimensions for the images, can be changed\n",
        "directory_path = 'sfddata/sign_data/train/'\n",
        "subdirectories = [\"049\", \"049_forg\"]\n",
        "\n",
        "# Loop over the subdirectories\n",
        "for subdirectory in subdirectories:\n",
        "    curr_directory = directory_path + subdirectory # where the image files are located\n",
        "    image_path = 'converted_images/train/' + subdirectory # where the images will be stored\n",
        "    if subdirectory == \"049\":\n",
        "      reduced_path = 'reduced/train/forgery'\n",
        "    else:\n",
        "      reduced_path = 'reduced/train/genuine'\n",
        "\n",
        "    gray_images = convert_grayscale(curr_directory)\n",
        "    resize(gray_images, dimensions)\n",
        "    save_images = True\n",
        "\n",
        "    os.makedirs(image_path, exist_ok=True)\n",
        "\n",
        "    if save_images:\n",
        "        for i in tqdm(range(0, len(gray_images))):\n",
        "            # Saves the bitmap images\n",
        "            image_filename = os.path.join(image_path, f\"image{i}.png\")\n",
        "            gray_images[i].save(image_filename)\n",
        "    # need to do post resize\n",
        "    reduce_noise(image_path, reduced_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DM4KdLDhATl"
      },
      "source": [
        "## Split Train Images into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Fys3UPhEh1",
        "outputId": "b45c3631-1050-423a-f93c-b4db62029286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24 files belonging to 2 classes.\n",
            "Using 20 files for training.\n",
            "Using 4 files for validation.\n",
            "[0.]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import image_dataset_from_directory\n",
        "import tensorflow_datasets as tfds\n",
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 2232\n",
        "IMG_HEIGHT = 250\n",
        "IMG_WIDTH = 250\n",
        "\n",
        "# Set directory to pull images from\n",
        "DATA_DIR = pathlib.Path('/content/reduced/train')\n",
        "\n",
        "\n",
        "# Make training & validation tensorflow datasets stored in list\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels=\"inferred\",\n",
        "    label_mode='binary',\n",
        "    validation_split=0.18,\n",
        "    subset=\"both\",\n",
        "    shuffle = True,\n",
        "    seed=1107,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "dataset_array = tfds.as_numpy(train_ds)\n",
        "\n",
        "train_dataset = dataset_array[0]\n",
        "val_dataset = dataset_array[1]\n",
        "\n",
        "for images, labels in dataset_array[0]:\n",
        "  np.save(\"/content/train_labels.npy\", labels)\n",
        "  np.save(\"/content/train_imgs.npy\", images)\n",
        "\n",
        "for images, labels in dataset_array[1]:\n",
        "  np.save(\"/content/val_labels.npy\", labels)\n",
        "  np.save(\"/content/val_imgs.npy\", images)\n",
        "\n",
        "print(labels[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "G8B02Jv3o6aj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL3NQwo3o6aw"
      },
      "source": [
        "## Convert Image to Grayscale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93fe51bd-26f9-4038-f443-efd4c9d7ebd6",
        "id": "7GDi_LPjo6ax"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:00<00:00, 186.18it/s]\n",
            "100%|██████████| 12/12 [00:00<00:00, 94.83it/s] \n"
          ]
        }
      ],
      "source": [
        "\"\"\"Code is used for processing images\"\"\"\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import numpy\n",
        "\n",
        "THRESHOLD = 128\n",
        "\n",
        "def image_to_grayscale(image_dir: str) -> Image:\n",
        "    \"\"\"Used for testing purposes to convert one image to grayscale\"\"\"\n",
        "    image = Image.open(image_dir)\n",
        "    gray_image = ImageOps.grayscale(image)\n",
        "    return gray_image\n",
        "\n",
        "def convert_grayscale(directory: str) -> list:\n",
        "    \"\"\"Converts all images in the given directory into gray scale\"\"\"\n",
        "    converted_images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".png\") or filename.endswith(\".PNG\"):\n",
        "            image = Image.open(directory + \"/\" + filename)\n",
        "            gray_image = ImageOps.grayscale(image)\n",
        "            converted_images.append(gray_image)\n",
        "    return converted_images\n",
        "\n",
        "def reduce_noise(directory: str, image_path: str) -> list:\n",
        "    \"\"\"Runs fastNLMeansDenoising on each img\"\"\"\n",
        "    # reduced_dir = \"/content/reduced/\" + directory\n",
        "    os.makedirs(image_path, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".png\") or filename.endswith(\".PNG\"):\n",
        "          noise_pic = cv2.imread(directory + \"/\" + filename)\n",
        "          image_again = numpy.asarray(noise_pic)\n",
        "          less_noise_pic = cv2.fastNlMeansDenoising(image_again, None, 15, 7, 21)\n",
        "          curr_path = os.path.join(image_path, f\"{filename}\")\n",
        "          cv2.imwrite(curr_path, less_noise_pic)\n",
        "\n",
        "def resize(images: list, dimensions: tuple) -> None:\n",
        "    \"\"\"Resizes all the given images in a list\"\"\"\n",
        "    for i in range(0, len(images)):\n",
        "        image = images[i]\n",
        "        images[i] = image.resize(dimensions)\n",
        "\n",
        "# actual code to run\n",
        "dimensions = (250, 250) # dimensions for the images, can be changed\n",
        "directory_path = 'sfddata/sign_data/test/'\n",
        "subdirectories = [\"049\", \"049_forg\"]\n",
        "\n",
        "# Loop over the subdirectories\n",
        "for subdirectory in subdirectories:\n",
        "    curr_directory = directory_path + subdirectory # where the image files are located\n",
        "    image_path = 'converted_images/test/' + subdirectory # where the images will be stored\n",
        "    if subdirectory == \"049\":\n",
        "      reduced_path = 'reduced/test/forgery'\n",
        "    else:\n",
        "      reduced_path = 'reduced/test/genuine'\n",
        "\n",
        "    gray_images = convert_grayscale(curr_directory)\n",
        "    resize(gray_images, dimensions)\n",
        "    save_images = True\n",
        "\n",
        "    os.makedirs(image_path, exist_ok=True)\n",
        "\n",
        "    if save_images:\n",
        "        for i in tqdm(range(0, len(gray_images))):\n",
        "            # Saves the bitmap images\n",
        "            image_filename = os.path.join(image_path, f\"image{i}.png\")\n",
        "            gray_images[i].save(image_filename)\n",
        "    # need to do post resize\n",
        "    reduce_noise(image_path, reduced_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIlNXPVco6ay"
      },
      "source": [
        "## Use Tensorflow Datasets to make Test npys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d447ed16-e69e-49ec-a8f8-e98a41c6e52a",
        "id": "qBWqTGrOo6ay"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24 files belonging to 2 classes.\n",
            "[1.]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import image_dataset_from_directory\n",
        "import tensorflow_datasets as tfds\n",
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 2232\n",
        "IMG_HEIGHT = 250\n",
        "IMG_WIDTH = 250\n",
        "\n",
        "# Set directory to pull images from\n",
        "DATA_DIR = pathlib.Path('/content/reduced/test')\n",
        "\n",
        "\n",
        "# Make training & validation tensorflow datasets stored in list\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels=\"inferred\",\n",
        "    label_mode='binary',\n",
        "    validation_split=0,\n",
        "    shuffle = True,\n",
        "    seed=1107,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "dataset_array = tfds.as_numpy(train_ds)\n",
        "\n",
        "for images, labels in dataset_array:\n",
        "  np.save(\"/content/test_labels.npy\", labels)\n",
        "  np.save(\"/content/test_imgs.npy\", images)\n",
        "\n",
        "print(labels[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsF98iTsHlQo"
      },
      "source": [
        "#Export Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "D-NFTMSlHnEB",
        "outputId": "e6ea2b64-f828-47a8-90b4-8b4b6fffbbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24, 1)\n",
            "(24, 250, 250, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7d9a4a5c-8cd5-4510-91e8-e5300997948a\", \"test_labels.npy\", 224)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8bf3ab6c-6aef-4bc2-aa3e-0175cbe89bed\", \"test_imgs.npy\", 18000128)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "test_labels = numpy.load(\"/content/test_labels.npy\")\n",
        "print(test_labels.shape)\n",
        "test_imgs = numpy.load(\"/content/test_imgs.npy\")\n",
        "print(test_imgs.shape)\n",
        "\n",
        "# download label arrays\n",
        "# files.download(\"/content/train_labels.npy\")\n",
        "files.download(\"/content/test_labels.npy\")\n",
        "# files.download(\"/content/val_labels.npy\")\n",
        "\n",
        "# download img arrays\n",
        "files.download(\"/content/test_imgs.npy\")\n",
        "# files.download(\"/content/train_imgs.npy\")\n",
        "# files.download(\"/content/val_imgs.npy\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "L1U3fGRQgDYO",
        "pqxk-hwAic85",
        "It8iUJU1g3hF",
        "1DM4KdLDhATl"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
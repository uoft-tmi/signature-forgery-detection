{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1U3fGRQgDYO"
      },
      "source": [
        "#Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_b_YWzhq9VS"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle Library\n",
        "!pip install kaggle\n",
        "\n",
        "# Before next step, user needs to download the free API KEY from Kaggle settings\n",
        "# Upload the kaggle.json file to Google Colab Files\n",
        "\n",
        "# Make directory for Kaggle & Refer to API KEY\n",
        "! mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download Dataset\n",
        "! kaggle datasets download shreelakshmigp/cedardataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld1wPTXYrCsA"
      },
      "outputs": [],
      "source": [
        "# ! mkdir sfddata\n",
        "! unzip cedardataset.zip -d sfddata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAStBcqKgwrJ"
      },
      "source": [
        "#Combine Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJagwfbTgobP",
        "outputId": "d5377756-2d01-4f00-ecfe-2d40c827897c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error creating directory 'signatures_combined': [Errno 2] No such file or directory: 'signatures_combined'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Paths to directories\n",
        "real_sig_dir = '/content/sfddata/signatures/full_org'\n",
        "fake_sig_dir = '/content/sfddata/signatures/full_forg'\n",
        "\n",
        "# Define the destination folder for the combined dataset\n",
        "destination_folder = 'signatures_combined'\n",
        "label_location = \"/content/signatures_combined/og_labels.npy\"\n",
        "\n",
        "# delete old stuff in dest folder if applicable\n",
        "try:\n",
        "    shutil.rmtree(destination_folder)\n",
        "except OSError as e:\n",
        "    print(f\"Error creating directory '{destination_folder}': {e}\")\n",
        "\n",
        "# Ensure the destination folder exists\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "# List the files in the source subfolders\n",
        "files1 = os.listdir(real_sig_dir)\n",
        "files2 = os.listdir(fake_sig_dir)\n",
        "\n",
        "# The list of results\n",
        "labels = []\n",
        "\n",
        "# Copy files from the first subfolder to the destination\n",
        "for file in files1:\n",
        "    source_file = os.path.join(real_sig_dir, file)\n",
        "    destination_file = os.path.join(destination_folder, file)\n",
        "    shutil.copy(source_file, destination_file)\n",
        "    labels.append(1)\n",
        "\n",
        "# Copy files from the second subfolder to the destination\n",
        "for file in files2:\n",
        "    source_file = os.path.join(fake_sig_dir, file)\n",
        "    destination_file = os.path.join(destination_folder, file)\n",
        "    shutil.copy(source_file, destination_file)\n",
        "    labels.append(0)\n",
        "\n",
        "sorted_labels = np.array(labels)\n",
        "# save images somewhere\n",
        "np.save(label_location, sorted_labels, allow_pickle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJup25ZtSqKR"
      },
      "source": [
        "#Convert Image to Grayscale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTBDjNcfSnO4",
        "outputId": "b03c3dc2-29f3-46cf-e789-5a627305e020"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2640/2640 [00:36<00:00, 72.31it/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Code is used for processing images\"\"\"\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "THRESHOLD = 128\n",
        "\n",
        "def image_to_grayscale(image_dir: str) -> Image:\n",
        "    \"\"\"Used for testing purposes to convert one image to grayscale\"\"\"\n",
        "    image = Image.open(image_dir)\n",
        "    gray_image = ImageOps.grayscale(image)\n",
        "    return gray_image\n",
        "\n",
        "def convert_grayscale(directory: str) -> list:\n",
        "    \"\"\"Converts all images in the given directory into gray scale\"\"\"\n",
        "    converted_images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".png\"):\n",
        "            image = Image.open(directory + \"/\" + filename)\n",
        "            gray_image = ImageOps.grayscale(image)\n",
        "            converted_images.append(gray_image)\n",
        "    return converted_images\n",
        "\n",
        "def resize(images: list, dimensions: tuple) -> None:\n",
        "    \"\"\"Resizes all the given images in a list\"\"\"\n",
        "    for i in range(0, len(images)):\n",
        "        image = images[i]\n",
        "        images[i] = image.resize(dimensions)\n",
        "\n",
        "# actual code to run\n",
        "dimensions = (250, 250) # dimensions for the images, can be changed\n",
        "directory = 'signatures_combined'  # where the image files are located\n",
        "image_path = 'converted_images'  # where the  images will be stored\n",
        "gray_images = convert_grayscale(directory)\n",
        "# bitmap_images = convert_bitmap(gray_images)\n",
        "resize(gray_images, dimensions)\n",
        "save_images = True\n",
        "\n",
        "if not os.path.exists(image_path):\n",
        "    # If the image path directory does not exist, create it.\n",
        "    !mkdir converted_images\n",
        "else:\n",
        "  shutil.rmtree(image_path)\n",
        "  !mkdir converted_images\n",
        "\n",
        "if save_images:\n",
        "    for i in tqdm(range(0, len(gray_images))):\n",
        "        # Saves the bitmap images\n",
        "        image_filename = os.path.join(image_path, f\"image{i}.png\")\n",
        "        gray_images[i].save(image_filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqxk-hwAic85"
      },
      "source": [
        "#Reduce Noise In Images & Save Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gAlhNxLikz9",
        "outputId": "65d62fde-bf5b-4f19-a24b-cbd724e38b64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-66-fe89619b7ec5>:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘reduced’: File exists\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2640/2640 [10:49<00:00,  4.06it/s]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from matplotlib import pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pathlib\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "# Get directory to access bitmap images\n",
        "dir = pathlib.Path('/content/converted_images')\n",
        "\n",
        "# Extract bitmap images and store in list\n",
        "pictures = list(dir.glob('*.png'))\n",
        "\n",
        "# Convert filenames to str and store in list\n",
        "images = []\n",
        "for pic in pictures:\n",
        "  images.append(str(pic))\n",
        "\n",
        "# where to store the images w reduced noise\n",
        "reduced_dir = \"/content/reduced\"\n",
        "!mkdir reduced\n",
        "\n",
        "compare_images = []\n",
        "less_noise_pics = []\n",
        "\n",
        "# Reduce noise in each bitmap image and store in list\n",
        "for i in tqdm(range(len(images))):\n",
        "  image = images[i]\n",
        "  noise_pic = cv2.imread(image)\n",
        "  image_again = asarray(noise_pic)\n",
        "  less_noise_pic = cv2.fastNlMeansDenoising(image_again, None, 15, 7, 21)\n",
        "\n",
        "  # images stored in tuple form => (original image, noise reduced image)\n",
        "  compare_images.append((noise_pic, less_noise_pic))\n",
        "  less_noise_pics.append(less_noise_pic)\n",
        "  curr_path = os.path.join(reduced_dir, f\"image{i}.png\")\n",
        "  cv2.imwrite(curr_path, less_noise_pic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It8iUJU1g3hF"
      },
      "source": [
        "#Split Data into Train & Test Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u3d2_M1g2s1",
        "outputId": "2b13ebe4-20e3-49e9-c912-fc093d2659cb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "data_path_train = \"/content/reduced\"\n",
        "! mkdir split\n",
        "data_path_test = \"/content/split\"\n",
        "\n",
        "# path to destination folders\n",
        "train_folder = os.path.join(data_path_test, 'training')\n",
        "test_folder = os.path.join(data_path_test, 'testing')\n",
        "\n",
        "# Define a list of image extensions\n",
        "image_extensions = '.png'\n",
        "\n",
        "# Create a list of image filenames in 'data_path'\n",
        "imgs_list = [filename for filename in os.listdir(data_path_train) if os.path.splitext(filename)[-1] in image_extensions]\n",
        "\n",
        "# Sets the random seed\n",
        "random.seed(1107)\n",
        "\n",
        "# Shuffle the list of image filenames\n",
        "indices = [x for x in range(0, len(imgs_list))]\n",
        "random.shuffle(indices)\n",
        "\n",
        "# determine the number of images for each set\n",
        "train_size = int(len(imgs_list) * 0.85)\n",
        "test_size = int(len(imgs_list) * 0.15)\n",
        "\n",
        "# Create destination folders if they don't exist\n",
        "if not os.path.exists(train_folder):\n",
        "    os.makedirs(train_folder)\n",
        "if not os.path.exists(test_folder):\n",
        "    os.makedirs(test_folder)\n",
        "\n",
        "# Rearrange the image files and labels\n",
        "new_imgs_list = [x for x in range(0, len(imgs_list))]\n",
        "new_labels = [x for x in range(0, len(imgs_list))]\n",
        "labels = numpy.load(\"/content/signatures_combined/og_labels.npy\")\n",
        "\n",
        "for i, j in enumerate(indices):\n",
        "  new_imgs_list[i] = imgs_list[j]\n",
        "  new_labels[i] = labels[j]\n",
        "\n",
        "# Store test imgs\n",
        "test_imgs = []\n",
        "\n",
        "# Copy image files to destination folders\n",
        "for i, f in enumerate(imgs_list):\n",
        "    if i < train_size:\n",
        "        dest_folder = train_folder\n",
        "        shutil.copy(os.path.join(data_path_train, f), os.path.join(dest_folder, f))\n",
        "    else:\n",
        "        dest_folder = test_folder\n",
        "        # add to test array\n",
        "        pic = cv2.imread(os.path.join(data_path_train, f))\n",
        "        test_imgs.append(asarray(pic))\n",
        "\n",
        "\n",
        "# Save labels\n",
        "train_labels = np.array(new_labels[:train_size])\n",
        "test_labels = np.array(new_labels[:train_size])\n",
        "# save images somewhere\n",
        "np.save(\"/content/og_train_labels.npy\", train_labels, allow_pickle=False)\n",
        "np.save(\"/content/test_labels.npy\", train_labels, allow_pickle=False)\n",
        "\n",
        "# Save test imgs\n",
        "test_imgs = np.array(test_imgs)\n",
        "np.save(\"/content/test_imgs.npy\", test_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DM4KdLDhATl"
      },
      "source": [
        "#Split Train Images into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Fys3UPhEh1",
        "outputId": "30e80c72-b52b-4a27-f705-7418ee7137fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2244\n",
            "[1 0 0 ... 0 0 1]\n",
            "Found 2244 files belonging to 2 classes.\n",
            "Using 1841 files for training.\n",
            "Using 403 files for validation.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import image_dataset_from_directory\n",
        "import tensorflow_datasets as tfds\n",
        "import pathlib\n",
        "\n",
        "BATCH_SIZE = 2232\n",
        "IMG_HEIGHT = 180\n",
        "IMG_WIDTH = 180\n",
        "\n",
        "# Set directory to pull images from\n",
        "DATA_DIR = pathlib.Path('/content/split/training')\n",
        "\n",
        "paths = len(list(DATA_DIR.glob('*.png')))\n",
        "print(paths)\n",
        "\n",
        "# get labels\n",
        "my_labels = numpy.load(\"/content/og_train_labels.npy\")\n",
        "print(my_labels)\n",
        "\n",
        "# Make training & validation tensorflow datasets stored in list\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    labels=my_labels.tolist(),\n",
        "    label_mode='binary',\n",
        "    validation_split=0.18,\n",
        "    subset=\"both\",\n",
        "    shuffle = True,\n",
        "    seed=1107,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "dataset_array = tfds.as_numpy(train_ds)\n",
        "\n",
        "train_dataset = dataset_array[0]\n",
        "val_dataset = dataset_array[1]\n",
        "\n",
        "for images, labels in dataset_array[0]:\n",
        "  np.save(\"/content/train_labels.npy\", labels)\n",
        "  np.save(\"/content/train_imgs.npy\", images)\n",
        "\n",
        "for images, labels in dataset_array[1]:\n",
        "  np.save(\"/content/val_labels.npy\", labels)\n",
        "  np.save(\"/content/val_imgs.npy\", images)\n",
        "# Testing folders has 15% of data but does not go through splitting\n",
        "# using tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YMQ9myNczut"
      },
      "source": [
        "# Example to Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCfAZkh41x8F"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "train_labels = numpy.load(\"/content/train_labels.npy\")\n",
        "train_imgs = numpy.load(\"/content/train_imgs.npy\")\n",
        "\n",
        "val_labels = numpy.load(\"/content/val_labels.npy\")\n",
        "val_imgs  = numpy.load(\"/content/val_imgs.npy\")\n",
        "\n",
        "test_labels = numpy.load(\"/content/test_labels.npy\")\n",
        "test_imgs = numpy.load(\"/content/test_imgs.npy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsF98iTsHlQo"
      },
      "source": [
        "#Export Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "D-NFTMSlHnEB",
        "outputId": "8656403f-f2c1-41c2-ef63-e38a01eee1f4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_eb6849c3-dc59-408b-bad7-21683cef5fba\", \"train_imgs.npy\", 715780928)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_2c886d4a-7e57-4acd-ac24-614752a3ec14\", \"val_imgs.npy\", 156686528)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/test_imgs.npy\")\n",
        "files.download(\"/content/train_imgs.npy\")\n",
        "files.download(\"/content/val_imgs.npy\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "L1U3fGRQgDYO",
        "QAStBcqKgwrJ",
        "It8iUJU1g3hF",
        "1DM4KdLDhATl",
        "xJup25ZtSqKR",
        "pqxk-hwAic85"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
